Perception and time in Artificial General Intelligence.
Andrey Makushkin
toandrey@yahoo.com
April 24, 2019


Summary:
Intelligence is an emergent behavior attributed to entities possessing mechanisms such as perception, short and long term memory, ability to learn or forget, attention, habituation, prediction, reflexes, embodiment, actuation and many other.  Perception is considered the most important mechanism by many researchers.[1][2]

In traditional computing, data is received or acquired and transformed by computers.  The output of these computations is interpreted or perceived by humans.  If a human is out of the loop, it's usually the case where certain output symbols are assigned actions by system designers.  When building Artificial General Intelligence (AGI) our goal is to move this perception boundary inside the system.

The goal of this paper is to define perception in the discipline of artificial intelligence (AI).  Perception is a low level mechanism that can be composed to form higher level functions.  Perception is the detection of an internal state change described in terms of time.  Detection of state change inside the system should not be described by further changing the system state.  This paper tries to explain this statement through examples and references to the existing literature as well as elaborate on the implications.  For example, above definition makes it possible to think about an observer as a process and not an object.  Another goal is to underline the importance of studying time aspects of computational mechanisms.  For example, modeling artificial neural network (ANN) synapses as oscillators or processes instead of weights.


Introduction:
In the fields of AI and AGI, currently accepted definition of perception can be summarized as "perception is a change in internal state as a result of external stimuli".  Better descriptions take time into account usually by including something abstract, for example, temporal patterns of neural activity.  These definitions do not adequately describe the mechanism.

A rock's internal state can be changed by heating it.  It will have a short term memory of it because it remains hot for some time.  It can form dazzling patterns and crystalline structures. Reflect or absorb light, conduct or insulate electric currents.  Geologists make careers out of studying long term memories of the rocks.  The rock can also actuate by falling as a result of changing its state.  Taking all that complex behavior into account not many will claim that the rock can perceive its environment.  This means CCD and CMOS sensors which are designed to sense the environment and whose wafers are made from silica are not able to perceive the environment.  On the other hand, many will agree that neurons, biological sensors, are able to perceive their environment.  Therefore inside a neuron a different process takes place.  Neuron's state (its membrane potential) is changed by external stimuli.  External stimuli constitute sensory stimuli or synaptic potential.  When the membrane potential reaches a certain voltage level it is detected and the neuron fires.  Detection is described by when it occurred as opposed to how the internal state changed.  This allows neurologists to model neural activity as one dimensional point processes with time labeling the axis.  Neuron's detection mechanism is significantly different from the processes occuring in the rock.

The following will describe the detection mechanism in view of different disciplines.  Some of the arguments found here will use an idea of a non-conscious observer borrowed from physics.  It will also rely on the notion of an object's state - the information contained within the object that describes it.  The author will sometimes refer to an observer's own internal state.  Information that describes the observer and most importantly how that information is changed by its environment.


Measure, values, numbers, symbols, state, perception and philosophy:
When we are looking at observations expressed in terms of values we are frequently looking at the number of events per unit of time.  A count of how many times a measuring stick fits.  How many changes have been detected.  Measuring the state of the environment produces values such as length, angles or voltage levels, frequently expressed as numbers in digital systems.  Numbers are measures of an object's state (or rate of state change) from an absolute reference point such as zero.  The reference point and scale usually have nothing to do with the object whose properties are measured.  They are qualities of an observer.  I measure in inches, you measure in centimeters. I measure in degrees, you measure in radians.  Does that thermometer measure the temperature in Fahrenheit or Celsius?  What is the difference between 20 and 68?  None, because 20 degrees Celsius is 68 degrees Fahrenheit.  Is five longer than five?  Yes, because five inches are longer than five centimeters.  Values are meaningless without units.  If the units do not match, things cannot be compared.

This leads to a conclusion that although a measure might have a meaning for a single observer, it is meaningless to another observer unless units are agreed upon.  Ironically quantification, a concept that has to give meaning to observation is the reason observations lose their grounding.[5]  Quantification forms a symbolic world representation.  It would be easier to understand perception through a mechanism where the environment changes an observer's state without agreeing on units than transferring information in terms of numbers or symbols.  The observer simply adapts to how its state changes.

Before proceeding, let me reference some historical views on perception. The first well-known models of perception are Plato's theory of forms and his "Allegory of the Cave".  Plato describes objects being perceived as shadows without an observer ever knowing their true form.  He also adds that a true philosopher thinks in terms of forms.

Numbers are projections of real-world phenomena onto an axis.  They are Plato's shadows.  Every time a measurement is made, a shadow is produced.  The alternatives are Plato's forms.  Forms cannot be measured, described as a symbol, or transmitted.  Forms can only be detected.  This closely relates to the problem of qualia which cannot be expressed symbolically.  Qualia is a perception of a form.  Biological neurons detect Plato's forms.

Any observer cannot directly measure the state of some object such as its color or temperature.  It can only observe changes in the environment.  The object of observation does not always need to change.  A change in the medium is sufficient to deliver the informaton to the observer.  A photon arriving at the retina, hair deflecting in a cochlea, tactile input, molecule locking into a receptor in the nose, they are all events occurring in the sensor's environment.

Whenever a change occurs it has to be described in terms of time.  For example, a drop in temperature freezes the lake, not the cold itself.  The fact that it's cold can tell us about the state of the lake being frozen. It does not tell us how long.  Only the fact that the temperature is changing can describe that the state of the lake is changing.

Meaning of the information changes over time.  Information that there is a wall ten feet ahead might be meaningless to a self driving car a year after because by then it could have been involved in an accident and recycled into a toaster.  If information is represented by a symbol, a millisecond later a different symbol has to be used because the information has already changed.  This is one more reason to express information in terms of time.

Time instance is the smallest quanta of information yet since time is continuous its resolution is limited only by an observer's precision.  Detecting a change is much more precise in time than sampling.  While sampling, the change could occur anywhere within the time period.  There is another side effect of using a detection mechanism.  Detecting a change will result in low jitter and therefore a constant delay while it propagates through the system from an observer to another observer.  Similar to the propagation of the information about an event through the environment.  These delays can be exploited by some mechanisms, for example, when locating a source of sound.  Another example is mercury delay lines which were used in the early days of electronics to create computer memory.


Perception in Artificial Intelligence (AI):
First memorable mention of perception in AI has to be attributed to Frank Rosenblatt.  In his 1957 paper "The Perceptron A Perceiving And Recognizing Automaton" he puts perception first in the list of "human-like functions". He also references Plato's' forms. "the form of a man seen from any angle"  This reference has been greatly overlooked along with his mention of inhibiting connections not included in the current perceptron definition. He writes about his "Sensory System" which we now call the input layer "any connection may be negative (tending to inhibit)". In addition, Rosenblatt emphasizes the importance of time by dividing perceptrons into two categories: "momentary stimulus perceptrons and temporal pattern perceptrons".

Whereas Rosenblatt wanted his perceptrons to detect the forms, nowadays we have a more generic word for mechanisms of perception to detect forms.  They are called features.  A feature is a mechanism of perception.  Features can be detected by current computation models and even used in a generative mode to create pictures.  Features can be used in a generative mode because they embed information into its definition.  Feature's importance as a mechanism of perception that can make real-world observations in terms of time is not well-known.

Are features the answer to perception?  It depends on your point of view.  In many current models feature detection or absence is represented by symbols 1 and 0.  Some researchers think about these models of ANNs as symbolic in nature where symbols 0 or 1 are used to "communicate" among neurons.
 The problem with this point of view is the "Symbol Grounding Problem".  Symbol grounding problem should not be just a red flag for symbolic AI but a red brick wall for any approach that uses state in its computation model.  Zeros and Ones used in ANNs are not symbols.  They are flags representing short intervals of time.  There are problems with this way of representing time.  Some of the problems are described in "Finding Structure In Time" paper by Jeffrey L. Elman.  The important thing is to agree they represent time.  By doing so they avoid the "Symbol Grounding Problem" inherent in symbolic systems.

Another ANN related fact that underlines the importance of the time aspect of the computational models is the success of LSTM algorithm.  Its success can be attributed to a clever timing mechanism that makes it stand out among other recurrent ANNs.  The mechanism facilitates processing of stimuli from time intervals far apart.

If you train a supervised learning algorithm to give you a prediction one day into the future, it will not be able to predict what happens in an hour.  This fact is counterintuitive.  At first, it might appear that making an algorithm an online learning algorithm might solve the problem.  However, this is not the case and the problem should be looked at from a different angle.  There are two types of AI.  A static world, turn-based and a dynamic world AI.  In a dynamic world, processes are observed.  Time is a fundamental description of the perceived state of the process (signals).  In static world AI, a system is presented with data without a time component.  Such is done in turn-based games, pattern recognition, etc.  Static world AI can be tricked to perform on dynamic world problems by saying take turn every few milliseconds or run every time a sensor is read.  This approach can even be used in robotics.  However, it impedes progress by climbing local minima.


Neurology:
A hypothesis that brain is required for mobility is given to us courtesy of the sea squirt.  It turns out every voluntary answer your brain gives to any question involves muscle movements.[3]  We can look at a muscle fiber as an interface between a brain and the outside world.  A muscle fiber only needs to know if it has to contract right now.  The only question that the brain has to answer then becomes "should I twitch that muscle fiber right now?"  The answer is always "yes" meaning "contract right now" or it does not arrive at all.  "Now" is the best answer!  All information available before right now is taken into account.  Answering by providing a specific time or a condition that has to occur in the future runs a risk that new information will become available and a decision will no longer be optimal.  This proves the importance of perception in terms of time.

In biological NNs over two hundred neurotransmitters are found. If an analogy with ANN is drawn, over two hundred different messages or symbols can be sent or received by neurons.  Current theories do not address this variety of messages processed by biological neurons.  In neurology, mechanisms of changing the internal state of a neuron are many and variable.  Neuron's membrane potential can be changed by photons, chemical, electrical or mechanical means.  Frequently indirectly through the flow of sodium and potassium ions across a membrane.  Mechanisms can be inhibiting or exciting.  These mechanisms are important but are not common to all types of cells.  The common principle is the detection of internal state change.  How detection works also varies.  Adaptation indicates detection thresholds are not constant.[10]  On the other hand, pain neuronal pathways are not learned and  pain receptors do not become less sensitive over short time.[9]

Once the mechanism of perception is accepted, one can argue about the mechanisms of state change. Whether they require inhibition. If synapses should be modeled as weights or oscillators.  It might come down to the fact that messages passed between neurons or stimuli are not important.  What is important is that change in the internal state of the neuron is detected.  As a result, functionality of sensory neurons can be treated the same as cortical or peripheral neurons.  They are all change detectors.  In ANNs this would translate to input and hidden layer neurons having the same detection mechanism.  Analogous, in spiking NNs, spikes are not important.  They are simply representations of the detector's output.

Why detection?  Why does the observer need to detect its own state change?  What about other mechanisms?  Can an observer after its state has been modified go on to continuously modify another observer's state proportionally to its own state change?  Similar to analog electronics where for many components changing its input changes its output continuously.  This approach is used in analog computing which is also not symbolic in nature.  However, this approach leads to a continuous expansion of energy.  The detection mechanism is also a biological approach to minimizing energy consumption.  According to Rashevsky, "for a set of prescribed biological functions an organism has the optimal possible design with respect to economy of material used, and energy expenditure, needed for the performance of the prescribed function." [7]

It takes energy to change state. This energy should not be expanded unless the observer's state in the previous example has changed. Otherwise, by expanding this energy, the observer changes its own state nondeterministically.  However, a system needs to react to external stimuli and not just its unit's random internal state changes.  In addition, the observer needs to reset its state in such a way that when its state is changed again, the net change can be detected.


On the importance of time in physics and engineering:
In classical physics, one wants to describe the world in terms of functions.  For example, in an experiment where a ball is moving left and right on a track the ball's position can be described as a function of time f(t).  This description breaks down when something unaccounted for happens.  For example, the ball is picked up and later placed back on the track.  This action can be defined as a discontinuity in f(t).  The discontinuity can be described in terms of the time interval because math describes the world in an instance of time as a function of time.

In Engineering non-linear processes are often modeled by linear functions on some intervals.  The function can be chosen by an engineer or fitted by linear regression.  When linear approximation is used for a function of time, a time interval during which it is valid has to be defined.  This is similar to subsumption architecture in robotics where state machines describing non-linear behavior become valid during some periods of time.  Describing everything in a system (perception/ internal representations /actions) in terms of time fits well with the techniques described above.


Digital Signal Processing (DSP), probability and statistics:
How do you turn continuous into discrete?  Sample it at even time intervals to produce time series.  An alternative is to detect changes in the signal and record the amplitude and time of the change.  As the speed of an ADC converter increases, amplitude changes from previous value take on a set {-1,0,1} due to the limits of sensor resolution.  It becomes a change detector.  This fact is exploited in "One-bit Audio" encoding.

Sampling a random process at an arbitrary frequency does not guarantee exceeding Nyquist frequency of a real-world phenomenon. Oversampling at the maximum sensor frequency produces excessive amounts of data.  For example, a sensor sampling brightness 30 times per second produces 2,592,000 samples in 24 hours. This is excessive if the goal is to figure out whether it is night or day.  Even if data is not retained, it takes energy to process it. <EDIT: move sampling changes the statistics here???>

Observing the signal via multiple observers that detect changes instead of sampling is a better model that uses the mechanism of perception.  For example, observer1 detects a change in one unit from an absolute.  Observer2 detects a change in two units and so on.  When noise is filtered out, the approach of detecting the changes produces the optimal rate for capturing the signal.  Detecting signal changes guarantees to capture the information needed to reproduce the signal.  Detection mechanism does not suffer from sampling below Nyquist frequency.

In DSP, random variables (random processes) such as phase, amplitude and frequency are sometimes defined a priori.  Transmitter manipulates these variables creating a signal.  Whereas frequency is a known random variable when designing filters.  It is easy to fall into a trap thinking that using these random variables in any signal will provide some insight into the information content.  Others try to use statistical properties of the signal such as mean, median, deviation, distribution to gain insight.  The problem with this approach is that any complex signal is a composition of multiple random variables.  Without knowing all of them, it is impossible to understand how one of them contributes to the statistics of the signal.

There is an alternative approach of creating <EDIT probability???> statistical experiments to extract information.  The problem is, how does one construct an experiment automatically?  How does one go from observing a continuous random process to an event?  The mechanism of perception can be used to define and runs statistical experiments.  Detecting a change in a signal produces an experiment outcome whereas sampling it does not.  This is because in the case of sampling, the sample rate can change, changing the statistics.  <EDIT: bertrand here> The concept of feature binding helps define the boundaries of the experiment.  For different observers observing the same events, the experiments will be different.  This is how perception conducts experiments in the real world.  For example, dogs cannot see colors whereas we can.

At this point importance of an ability to actuate and embodiment has to be mentioned.  It enables modification of statistical experiments in addition to observing them.  <EDIt move and tie with sampling frequency instead > For example, in Bertrand paradox method of selection influences the distribution of the random variable.[8]  Another example is a coin laying on the road.  Cars are passing over it and push the coin around but it does not flip.  It always lays on one side.  Observing this experiment without an ability to modify it will not tell you this is a fair coin.

When working with data, researchers often face combinatorial explosion.  Having a time component may help reduce the number of possible combinations of random variable realizations.

Data may exhibit a correlation without causality.  Causality on the other side will express itself as a correlation in data from different time intervals.  To avoid an inverse error one needs to specify that correlation of data from different time intervals does not imply causality.  In order to reason about causality, one needs to establish the notion of time in the reasoning framework.  This is easier done when information is expressed in terms of time as for example in a one dimensional point process with time labeling the axis.


Conclusion:
It's important to revisit and reconsider classic research with this new outlook on perception to find what was missed and correct the assumptions.  Contemporary directions of research aligned with this view on perception include neuromorphic computing, reservoir computing, spiking ANNs, BEAM robotics, study of coupled oscillators and study of point processes.


References:
[1] Frank Rosenblatt, The Perceptron, 1957
[2] Stephen Wolfram, New Kind of Science,  chapter 10 "Perception"
[3] https://www.ted.com/talks/daniel_wolpert_the_real_reason_for_brains
[4] https://en.wikipedia.org/wiki/Point_process
[5] https://en.wikipedia.org/wiki/Quantification_(science)
[6] https://en.wikipedia.org/wiki/Symbol_grounding_problem
[7] Rashevsky, N., 1960. Mathematical Biophysics: Physico-Mathematical Foundations of Biology, vol. 2, p. 292
[8] https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)
[9] http://www.newworldencyclopedia.org/entry/Pain
[10] https://en.wikipedia.org/wiki/Neural_adaptation

TODO: change Wikipedia and ted.com links to links to papers


############### Should any of this info be included in the paper?  (As an appendix?) #######################
My speculation is that algorithms required to build an Intelligent system using ANNs would describe (A) how an individual neuron operates (detection/threshold and internal state change mechanism) (B) how neurons interconnect into groups such as cortical columns (C) how these groups are connected to each other, which might be a genetic algorithm responsible for reflexes.

Compare apples to oranges:
How about a "purple over here" and a "green over there"?  What units can we measure them with? Why do we separate the measures into different domains of color and space?  What if the observer is color blind?  For him, color is a completely foreign domain.  We have to have a domain-independent representation.  This need relates to concepts of symbol grounding and qualia.  The domain-independent representation can be accomplished by representing everything in terms of time.

Detection is a way of thinking about the mechanism of perception.  One of its forms is threshold detection.
Why consider detection mechanism? Assuming this model, creates a framework for an exhaustive search for rules of state modification. 
Algorithm of changing state changes itself over time. Synapses change over time.  Habituation gives us a hint that the detection algorithm can also change over time. How does the environment influence the change mechanism?   Why is there an inhibiting mechanism?

The assumption that state can describe perception is a result of the state-oriented view of the world as opposed to process-oriented. In the process-oriented view, a change in a process can be described by when it occurred as opposed to how the state changed.

A thermostat can detect a change in temperature.  It can make my AC actuate.  Does it perceive the change?

As a function terminates, twitch the muscle.  How does this relate to lambda calculus?

Manchester encoding adds observer clock synchronization to symbols used to communicate.

Machine learning deals with data bypassing perception

One cannot detect what he or she does not know.

The brightness of a pixel (red 34) VS one pixel is brighter than the other.  Which carries more information?

What action should I perform right now? And should I perform a specific action right now?  Both approaches use all available up to now information.

Medium is required to deliver a message.  The fact that new information is received is in itself a change.

Perception is a method of selection.  A method of observation of a random process.  It results in a point process.

It's easier to control a process if a control system is able to influence derivatives (how something changes over time) as it occurs in a PID controller.

################# statistics ##################################
Relative frequency of occurrence of an event is a measure of the probability in the frequentist interpretation.
find random variables hidden in a signal automatically ( reducing the dimensionality of the data? )

Random variable realization is not necessarily an experiment outcome.
Detection of changes <in the environment?> defines all possible outcomes of an experiment.
If it cannot be detected, it cannot become an outcome in the experiment.
Perceiving any continuous random process will yield discrete time observations - discrete mappings to a set of outcomes.
Each observer detects an outcome in an experiment.  The set of outcomes is defined for a short time period within the window of an experiment.
Detection described in terms of point processes form a set of possible outcomes.
Perception converts a continuous random process into a point process.
