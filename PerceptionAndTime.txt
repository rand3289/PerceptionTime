Perception and time in Artificial Intelligence.
Andrey Makushkin
toandrey@yahoo.com
February 18, 2019

Summary:
Perception is detection of an internal state change described in terms of time.  Detection of state change should not be described by changing the state.  This paper tries to explain this statement through examples and references to existing literature as well as elaborate on the implications.

Introduction:
Current widely accepted definition of perception can be summarized as "perception is a change in internal state as a result of external stimuli". Author will refrain from providing references to these definitions. I will simply provide a counter-example: a rock's internal state can be changed by heating it.  It will have a short term memory of it because it remains hot for some time. Geologist have made careers out of studying long term memories of the rocks. It can also actuate by falling on a passer-by's head as a result of changing its state. However I hope no one will claim that the rock is alive or can perceive it's environment.
On the other hand inside a neuron a different process takes place. Neuron's state (it's membrane potential) is changed by external stimuli. External stimula constitutes sensory stimula or other neurons firing. When membrane potential reaches a certain voltage level it is detected and neuron fires.

Misunderstanding that state can describe perception is a result of the state-oriented view of the world as opposed to process-oriented. In the process-oriented view a change in a process can be described by when it happened as opposed to <REWISE> a symbol changing from A to B in the state-oriented view. In this case symbol can represent any property or quantity.

Before proceeding to explain how this relates to Artificial Intelligence, lets review some historical views on perception. One of the first well known models of perception are Plato's theory of forms and his "Allegory of the Cave". Plato describes objects being perceived as shadows without ever knowing their true form. Modern computers are prisoners in Plato's cave who see shadows of the real world. What are these shadows? They are numbers!  Numbers are projections of real world penomena onto an axis. Every time a measurement is made, a shadow is produced. What are the alternatives you ask? In Plato's terms they are forms. Computers can perceive the world in terms of forms by detecting them.  Forms can not be measured, described as a symbol, or transmitted.  In the field of Artificial Intelligence, plato's forms are known as features.  A feature is a mechanism of perception.  Features can be detected by current models and even used in a generative mode to create  pictures.

Zeros and Ones used in ANNs are not symbols. They are flags that signify <belonging> to a time period.  There are problems with this way of representing time. Some of them are described in "<CHANGE> On Time" paper.  However the important thing is to understand they represent time.  By doing so they avoid the "Symbol Grounding Problem" inherent in symbolic systems.

Connectionism does not explain this. In addition it does not explain <insert reference to many neuro transmitters>

Are features the answer to perception? In many current models feature detection or absense is represented symbolically as 0 or 1. The problem with that is the "Symbol Grounding Problem".

Symbol grounding problem should not be just a red flag for symbolic AI but a red brick wall for any approach that uses state in it's computation model. <DELETE?> Current models of ANNs are also symbolic in nature.  Symbols 0 or 1 are used to "communicate" among neurons.  
This has been addressed previously (insert reference to paper "On time") 

In a biological neural network over two hundred neurotransmitters can be found. If an analogy with ANN is drawn, over two hundred different messages or symbols can be sent or received by neurons. Connectionism does not address this variety of symbols processed by biological neurons. Mechanisms of changing internal state of a neuron are many and variable. Neuron's membrane potential can be changed via photons, chemical, electrical or mechanical means.  
These mechanisms are important but are not common to all types of cells.
The common principle is the detection of internal state change.


Implications:

In the context of Artificial Neural Network (ANN), one might argue that hidden layers percieve their inputs just as external stimuli are prerceived. In this case messages passed between neurons are not important. What is important is that change in the internal state of the neuron is detected. This fact drastically influences the ANN models. 

Beam robotics, liquid state machines.
Aanalog computing is not symbolic in its nature.

Neurologists often model neural activity as one dimentional point processes with time labeling the axis.
LSTM is successful by implementing a clever timing mechanism.

Embodyment might be the whole reason for having a brain.
Importance of embodyment can be explained in terms of statistics: embodyment allows one to change the experiment instead of just observing it. Bertrand paradox Method of selection/realization/observation influences the distribution of the random variable.

Artificial intelligence is an emergent property attributed to entities posessing certain mechanisms such as perception, short and long term memory, ability to learn or forget, habbituation, attention, 

What action should I perform right now? vs Should I perform a specific action right now?
Both approarches try to answer what the sequence of actions should be.

Now is the best answer. All information available up to now has been considered.

Once the mechanism of perception is accepted, one can argue about the mechanisms of state change. Wether they require inhibition. If synapses should be modeled as weights or oscillators.


Statistics:
In statistics perception runs statistical experiments.  It allows one to go from observing random processes in real world to producing results of an experiment. Every time a change is detected, it produces an outcome in an experiment.

Imagine you are observing a toss of a fair coin in the air. There are several random processes that can be observed for example:
f(t) is the height of the coin in inches
g(t) is the hight of the coin in meters
h(t) is the angle of the coin relative to horizon in degrees
how does one create an experiment to check if the coin is fair? First one needs to pick values of the random processes that define an outcome of the experiment. Lets say f(t) = 0, g(t) = 0 and h(t) = 0.  In addition one needs to pick times t1, t2, t3, etc at which these random variables are realized.


################ from another file #############

a better description takes time into account by including patterns of temporal-spacial neural activation.

First memorable mention of perception in AI has to be attributed to Frank Rosenblatt.  In his 1957 paper "The Perceptron A receiving and recognizing automaton" he puts perception first in the list of "human-like functions". He also references Platos' forms. "the form of a man seen from any angle" However this reference has been greatly overlooked along with his mention of inhibiting connections not included in the current perceptron definition. He writes about his "Sensory System" which we now call the input layer "any connection may be negative (tending to inhibit)". In addition Rosenblatt emphasises importance of time by dividing perceptrons in two categories: "momentary stimulus perceptrons and temporal pattern perceptrons".

Where as Rosenblatt wanted his perceptrons to detect the forms, now days we have a more generic word for mechanisms of perception to detect forms.  They are called features.

Talk about qualia

Why detection? What is wrong with measuring? Measurement is a projection of a real world phenomenon onto a number line.  A shadow. Although a measure might have a meaning for a single observer, it is meaningless to another observer unless units are agreed upon.  It is easier to understand perception through a mechanism where the first observer changes second observer's state without agreeing on units than transfering information in terms of numbers.  Unless one can come up with a mechanism how two observers agree on the units.

Still why is detection needed? Can't the second observer after it's state has been modified by the first go on to modify the third's observer state proportionally to it's own state change? Why does the second observer need to detect it's own state change? Why can't it continuosly change the third observer's state? This is similar to analog electronics where changing component's imput changes it's output.
It takes energy to change a state. This energy should not be expanded unless the second observer's state has changed. Otherwise by expanding this energy second observer changes it's own state non-deterministically. 
However a system needs to react to external stimuli and not just it's unit's random internal state changes.  Also second observer needs to reset it's state in such a way that when it's state is changed again, the net change can be detected.

This could be just a biological approach to minimize energy consumption (see Rashevsky's principle of Optimal Design) 
[For a set of prescribed biological functions an organism has the optimal possible design with respect to economy of material used, and energy expenditure, needed for the performance of the prescribed function.]

Detection is a way of thinking about mechanism of perception.  One of it's form is threshold detection.
Why consider detection mechanism? Assuming this model creates a framework for an exaustive search for rules of state modification. 
Algorithm of changining state changes itself over time. Synapses change over time.  Habbituation gives us a hint that detection algorithm can also change over time. How does the environment influence the change mechanism? Why is there an inhibiting mechanism?  

On the importance of time in physics, engineering and other sciences:
In classical physics one wants to describe reality in terms of functions. For example in an experiment where a ball is moving left and right on a track the balls position is described as a function of time f(t). This description breaks down when something unaccouned for happens.  If the ball is picked up and later placed back on the track this can be defined as a discontinuity in f(t). This discontinuity can be described in terms of the time intervals.
In Engineering proceses are often modeled by linear function on intervals of time. This function can even be fitted by linear regression. When linear approximation is used a time interval has to be defined.  This is similar to subsumption architecture in robotics where state machines describing non-linear behavior become valid during a period of time. Describing everything in a system in terms of time (perception/ internal representations /actions) fits well with techniques described above.

######################## Paper Structure ##################

Introduction:
Artificial Intelligence is an emergent behavior of several mechanisms including perception, memory, etc...
<Perception is...>
<rock example here>

Philosophy:
<shapes detected (plato)>
Change in temperature freezes the lake not the cold itself.
Best answer is always now!

Neurology:
Biological NNs... neurons detect shapes.
<line 105 in ai.txt>
Sensory and hidden layer neurons functionality are the same.
<Muscle twitch>
Adaptation indicates detection thresholds are not constant.  On the other hand pain neuronal pathways are not learned and do not degrade over time.<citation wikipedia???>

AI / Machine Learning:
Features detect shapes
Symbolic world representation leads to symbol grounding problem.
<Rosenblat - perceptron>
<LSTM>

Mathematics:
<Measure>
<Numbers are shadows>
<5" vs 5 centimeters>

Statistics:
Perception mechanism bridges observation of random processes (random variables dependent on time) and statistical experiments.  Detection triggers the start of a new experiment. as well as defines it (feature binding concept helps define the boundaries of experiment (set of random variables) Detection described in terms of point processes form a set of possible outcomes.
* Embodyment allows changing the experiment
* Relative frequncy of occurence of event is a measure of the probability. <reference wikipedia ???>

Data without a time component produces combinatorial explosion. Having time reduces the number of possible combination of random variable realizations. Only data produced during identical time intervals can be combined.  This data will exibit correlation without causality. Causality on the other side will express itself as correlation in data from different time intervals. To avoid inverse error one needs to specify that correlation of data from different time intervals does not imply causality. Also one can NOT illiminate correlation without cause for example by dimentionality reduction techniques. In order to reason this way one needs to establish these time intervals. This is much easier to do when all information is expressed in terms of time instances.

DSP:
How do you turn continuous into discrete? Sample it at even time intervals to produce time series or detect changes in the signal and record the amplitude and time of the change or amplitude of the signal at time (t). As sensitivity of the detector increases, amplitude changes take on a set {-1,0,1} due to the limited resolution of sensors.  This fact is exploited in one bit audio <???> encoding.

Sampling at a random frequency does not guarantee to exceed Nyquist frequency. Oversampling at the maximum sensor frequency produces excessive amounts of data. Detecting signal changes after noise is filtered out guarantees to capture information needed to reproduce the signal.  For example a sensor capable of sampling brightness 100 times per second might produce 8,640,000 samples in a day. This is excessive if the goal is to figure out if it is night or day.

Conclusion:
The future directions of research aligned with my view on perception include neuromorphic computing, reservoir computing, BEAM robotics, study of coupled oscillators and point processes.  It is just as important to revisit and recosider classic reasearch with this new outlook on perception to find what was missed and correct assumptions.


< Conclusion or introduction ??? >
This paper's goal is to underline the importance of studying time askpects of computational mechanisms. For example modeling synapses as oscillators or processes instead of weights.
